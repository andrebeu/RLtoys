{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b3b9897",
   "metadata": {},
   "source": [
    "# REINFORCE on short corridor switched actions (Sutton & Barto Ch 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce2e0f2",
   "metadata": {},
   "source": [
    "## REINFORCE: Monte Carlo Policy Gradient\n",
    "1 ) generate episode\n",
    "\n",
    "2 ) loop over each timestep \n",
    "- compute return $G_t = \\sum_{k=t}^T \\gamma^{k-t-1} r_k $\n",
    "- calculate baselined target $\\delta_t = G_t - \\hat{v}(s;w) $\n",
    "- update value weights $ w_{t+1} = w_t + \\alpha_w \\delta_t \\nabla_w \\hat{v}(s;w) $ \n",
    "- update policy theta $ \\theta_{t+1} = \\theta_{t} + \\alpha_\\theta \\gamma^t \\delta_t \\nabla_\\theta log \\space \\pi (A_t|S_t;\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb6888",
   "metadata": {},
   "source": [
    "## simple derivation of policy gradient\n",
    "$ J(\\theta) \\equiv v_{\\pi_\\theta}(s_0) = E_{\\pi}[G_0|\\pi=\\pi_\\theta] $\n",
    "- let $\\tau = [(s_0,a_0),...,(s_T,a_T)] $ be some trajectory \n",
    "\n",
    "$ E_{\\pi}[G_0|\\pi=\\pi_\\theta] = \\sum_\\tau Pr(\\tau|\\pi_\\theta)G_0(\\tau) $\n",
    "- where $G_0(\\tau)$ is return on trajectory\n",
    "- take gradient and rearange\n",
    "    - note because trajectory conditioned on policy, trajectory return d/n depend on policy\n",
    "\n",
    "$\\nabla_\\theta \\sum_\\tau Pr(\\tau|\\pi_\\theta)G_0(\\tau) = \\sum_\\tau G_0(\\tau) \\nabla_\\theta Pr(\\tau|\\pi_\\theta) $\n",
    "\n",
    "- \"gradient of log trick\" follows from derivative of log and chain rule:\n",
    "\n",
    "$ \\nabla_\\theta log(Pr(\\theta)) = \\frac{1}{Pr(\\theta)} \\nabla_\\theta Pr(\\theta) $\n",
    "\n",
    "$ Pr(\\theta) \\nabla_\\theta log(Pr(\\theta)) = \\nabla_\\theta Pr(\\theta) $\n",
    "\n",
    "- substitute $\\nabla_\\theta Pr(\\tau|\\pi_\\theta)$ for $ Pr(\\tau|\\pi_\\theta) \\nabla_\\theta log(Pr(\\tau|\\pi_\\theta))$\n",
    "\n",
    "$ \\sum_\\tau G_0(\\tau) \\nabla_\\theta Pr(\\tau|\\pi_\\theta) =  \\sum_\\tau G_0(\\tau) Pr(\\tau|\\pi_\\theta) \\nabla_\\theta log(Pr(\\tau|\\pi_\\theta)) $\n",
    "\n",
    "- back to expectation \n",
    "\n",
    "$ = E [G_0(\\tau) \\nabla_\\theta log(Pr(\\tau|\\pi_\\theta)) |\\pi=\\pi_\\theta] $\n",
    "\n",
    "\n",
    "- reminder that $log(ab) = log(a) + log(b)$\n",
    "- and since $Pr(\\tau|\\pi_\\theta) = \\prod_k \\pi_\\theta(a_k|s_k)N(s_{k+1}|s_k,a_k) $\n",
    "- where $N(.|.)$ is environment transition, and sum is over trajectory elements\n",
    "- therefore\n",
    "\n",
    "$ \\nabla_\\theta log(Pr(\\tau|\\pi_\\theta)) = \\nabla_\\theta \\sum_k [log(\\pi_\\theta(a_k|s_k)) + log(N(s_{k+1}|s_k,a_k)) \\$\n",
    "\n",
    "- and becuase environment does not depend on policy:\n",
    "\n",
    "$ = \\sum_k \\nabla_\\theta \\pi_\\theta(a_k|s_k))$\n",
    "\n",
    "- therefore:\n",
    "\n",
    "$ = E [G_0(\\tau) \\nabla_\\theta log(\\pi_\\theta(a_k|s_k)) ] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d55d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as tr\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee21be",
   "metadata": {},
   "source": [
    "# env: short corridor with switched actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d256155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "  def __init__(self):\n",
    "    self.st = 0\n",
    "    self.t = 0\n",
    "    ## free param\n",
    "    self.maxt = 100 \n",
    "    None\n",
    "    \n",
    "  def __call__(self,action):\n",
    "    if self.st == 0:\n",
    "      if action:\n",
    "        stp = 1\n",
    "      else:\n",
    "        stp = 0\n",
    "    elif self.st == 1:\n",
    "      if action:\n",
    "        stp = 0\n",
    "      else:\n",
    "        stp = 2\n",
    "    elif self.st == 2:\n",
    "      if action:\n",
    "        stp = 3\n",
    "      else:\n",
    "        stp = 1\n",
    "    # update internal state\n",
    "    self.st = stp\n",
    "    reward = -1 \n",
    "    self.t += 1\n",
    "    if self.t >= self.maxt:\n",
    "      done = True\n",
    "    elif stp == 3:\n",
    "      done = True\n",
    "    else:\n",
    "      done = False\n",
    "    return stp,reward,done\n",
    "\n",
    "## helper\n",
    "def compute_returns(rewards,gamma=1.0):\n",
    "    \"\"\" \n",
    "    given rewards, compute discounted return\n",
    "    G_t = sum_k [g^k * r(t+k)]; k=0...T-t\n",
    "    \"\"\" \n",
    "    T = len(rewards) \n",
    "    returns = np.array([\n",
    "        np.sum(np.array(\n",
    "            rewards[t:])*np.array(\n",
    "            [gamma**i for i in range(T-t)]\n",
    "        )) for t in range(T)\n",
    "    ])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25caaca0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(ns,neps,kw):\n",
    "  \"\"\" \n",
    "  REINFORCE agent-environment logic\n",
    "  \"\"\"\n",
    "  score = np.zeros((ns,neps))\n",
    "  for seed in range(ns):\n",
    "    agent = Agent(**kw)\n",
    "    # episode loop\n",
    "    for ep in range(neps):\n",
    "      env = Env()\n",
    "      # trial loop\n",
    "      done = False\n",
    "      at,stp = 0,0\n",
    "      A = []\n",
    "      R = []\n",
    "      S = []\n",
    "      while not done:\n",
    "        st = stp\n",
    "        S.append(st)\n",
    "        stp,reward,done = env(at)\n",
    "        at = agent.act()\n",
    "        A.append(at)\n",
    "        R.append(reward)\n",
    "      # update\n",
    "      agent.update(S,A,R)\n",
    "      score[seed,ep] = np.sum(R)\n",
    "  \n",
    "  return score\n",
    "\n",
    "# score = run_experiment(1,1,{'alfa':[0.05,0.05],'bmode':'value'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c78406",
   "metadata": {},
   "source": [
    "# agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bfcfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(tr.nn.Module):\n",
    "  \n",
    "  def __init__(self,alfa,bmode):\n",
    "    super().__init__()\n",
    "    # learning rate\n",
    "    self.alpha_pi = alfa[0]\n",
    "    self.alpha_val = alfa[1]\n",
    "    # \n",
    "    self.build()\n",
    "    self.baseline = self._get_baseline(bmode)\n",
    "    # \n",
    "    return None\n",
    "  \n",
    "  def build(self):\n",
    "    # policy params\n",
    "    self.theta = tr.nn.Linear(1,2,bias=False)\n",
    "    self.theta.weight = tr.nn.Parameter(\n",
    "        tr.Tensor([[0],[5.]] # init greedy right\n",
    "      ),requires_grad=True)\n",
    "    # value params\n",
    "    self.vweights = tr.nn.Parameter(tr.Tensor([0]))\n",
    "    # optimization\n",
    "    self.optiop = tr.optim.Adam(\n",
    "      self.parameters(), \n",
    "      lr=self.alpha_pi\n",
    "    )\n",
    "    return None\n",
    "    \n",
    "  def forward(self):\n",
    "    return self.theta(tr.Tensor([1])).softmax(-1)\n",
    "\n",
    "  def act(self):\n",
    "    pr_right = self.forward()[0]\n",
    "    action = np.random.binomial(1,pr_right.detach().numpy())\n",
    "    return action\n",
    " \n",
    "  def _get_baseline(self,bmode):\n",
    "    if type(bmode)==type(None):\n",
    "      return lambda x: 0\n",
    "    if bmode=='value':\n",
    "      return lambda s: self.vweights\n",
    "  \n",
    "  def update_online(self,states,actions,rewards):\n",
    "    returns = compute_returns(rewards)\n",
    "    L = []\n",
    "    for St,At,Gt in zip(states,actions,returns):\n",
    "      ## compute \"loss\"\n",
    "      delta = Gt - self.baseline(St)\n",
    "      piAt = self.forward()[At]\n",
    "      los_pi = delta*tr.log(piAt)\n",
    "      # value loss\n",
    "      los_val = tr.square(self.vweights - Gt)\n",
    "       # update value step\n",
    "      self.optiop.zero_grad()\n",
    "      los_val.backward()\n",
    "      self.optiop.step()\n",
    "      # update pi step\n",
    "      self.optiop.zero_grad()\n",
    "      los_pi.backward()\n",
    "      self.optiop.step()\n",
    "      \n",
    "  def update_batch(self,states,actions,rewards):\n",
    "#     self.alpha_pi,self.alpha_val = [0.1,0.3]\n",
    "    returns = compute_returns(rewards)\n",
    "    L = []\n",
    "    los_pi,los_val = 0,0\n",
    "    for St,At,Gt in zip(states,actions,returns):\n",
    "      ## compute \"loss\"\n",
    "      delta = Gt - self.baseline(St)\n",
    "      piAt = self.forward()[At]\n",
    "      los_pi += delta*tr.log(piAt)/len(states)      \n",
    "      # value loss\n",
    "      los_val += tr.square(self.vweights - Gt)/len(states)\n",
    "    # update value step\n",
    "    self.optiop.zero_grad()\n",
    "    los_val.backward()\n",
    "    self.optiop.step()\n",
    "    # update pi step\n",
    "    self.optiop.zero_grad()\n",
    "    los_pi.backward()\n",
    "    self.optiop.step()\n",
    "    return None \n",
    "  \n",
    "  def update(self,states,actions,rewards):\n",
    "    return self.update_online(states,actions,rewards)\n",
    "\n",
    "  \n",
    "# a= Agent([0.05,0.05],None)\n",
    "# a.update([0],[0],[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0b77b8",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be8842",
   "metadata": {},
   "source": [
    "### learning rate experiment\n",
    "- without baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0315a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns,neps = 50,1000\n",
    "L = [-12,-13,-14]\n",
    "\n",
    "kwargs = {'bmode':None}\n",
    "\n",
    "for a in L:\n",
    "  kwargs['alfa'] = [2**a,0]\n",
    "  score = run_experiment(ns,neps,kwargs)\n",
    "  plt.plot(score.mean(0),label='alpha=2^%i'%a)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc53d89e",
   "metadata": {},
   "source": [
    "### baseline experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9ce6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.55 s ± 730 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "ns,neps = 50,1000\n",
    "\n",
    "aL = [[2**-12,0],[2**-9,2**-6]]\n",
    "mL = [None,'value']\n",
    "\n",
    "for alfa,bmode in zip(aL,mL):\n",
    "  kwargs = {'alfa':alfa,'bmode':bmode}\n",
    "  score = run_experiment(ns,neps,kwargs)\n",
    "  plt.plot(score.mean(0),label='baseline %s'%bmode)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24a97c",
   "metadata": {},
   "source": [
    "# notes\n",
    "### comparison between online versus batch updates:\n",
    "- batch: 1.79 s ± 46 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "- online: 8.55 s ± 730 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "    - NB: batch benefits from larger alpha because of fewer updates \n",
    "        - sample average makes gradient estimate more stable\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
